---
title: "t-test"
output:
  bookdown::html_document2:
    includes:
      in_header: assets/07_tTest_image.html
      after_body: assets/foot.html
---


:::obj
**Module learning objectives**

1. Determine how to quantify the uncertainty of an estimate
1. Describe the concept of statistical inference 

:::

# The Two Islands

You'd like to make an [inference](06_standardError.html) to formally investigate whether you have any reason to believe that the mean of heights on Island 1 differs from the mean of the heights on Island 2. 

The means of the heights of both islands are shown below:

```{r, echo= FALSE} 

set.seed(12)
heights_island1 <- rnorm(50, 10, 2)
heights_island2 <- rnorm(50, 18, 1.2)

```


```{r}
mean(heights_island1)
mean(heights_island2)

```

It's obvious that the mean of the two **samples** we took are different, but the main question is if the **population** means are different?

This question is of interest because if the two population means are different, it may indicate that a cool evolutionary story is at play. For example, based on your experience it seems clear that the two groups of giraffes have been isolated. Their tiny stature would make it near impossible to move back and forth between the two islands, hence impeding mixing between the two groups. Over time, selection pressures could then have made the two groups distinct regarding height.

How do we test this?

# Testing for group difference 

When testing for group mean differenes, [it is much more common for a researcher to be interested in the **difference** between means than in the specific values of the means themselves. ] stolen

When we focus on the **mean difference**, represented by $\Delta_{\bar{x}}$ (read "delta x-bar"), we can ask: is the $\Delta_{\bar{x}}$ meaningfully different from 0?

The sample mean difference is below:
```{r}
mean(heights_island2) - mean(heights_island1)

```

This is just another estimate at the sample level whose precision we need to quantify to be able to make inferences.

Formally, statistical inference is about testing hypotheses (which we intentionally did not introduce in the previous module). In research, a **hypothesis** is a statement of a suggested outcome for a study [THIS SENTENCE IS TOO VAGUE]. The goal of statistcal inference is to reject the **null hypotheses** ($H_0$, read "H-nought"), the default suggested outcome, which assumes that there is no association or difference between two or more groups. If we cannot reject $H_0$, then it means we must accept the alternative, $H_A$, that there *is* a meaningful difference or association.

Our null hypothesis is the following: the mean difference between the two populations is 0. The corresponding equations are shown below for the null and alternative hypotheses. Here, we use $\Delta_{\mu}$, because we are referring to the population values.

$H_0$ :  $\Delta_{\mu}$ = 0
<br> 
$H_A$ :  $\Delta_{\mu}$ $\neq$ 0
[MAKE EQUATION??]

One way to reject the null hypothesis would be to construct 95% CIs around the estimate for $\Delta_{\bar{x}}$.

# Pooled standard deviation
As we learned previously, a prerequisite to calculating 95% CIs is to calculate the standard error (which we know will require the sample standard deviation). However, we're now working with two samples, so which sample's standard deviation do we use? Can we use both?

In this particular case, the standard error of the mean difference can be calculated based on a *combined*, or **pooled standard deviation** ($s_{p}$) from two samples. We use the standard deviation of each sample, weighted by sample size.


<div style="margin-bottom:50px"></div>
\begin{equation}
 (\#eq:equation1)
 \Large s_{p} = \sqrt{\frac{s^2(n_1-1) + s^2(n_2-1)}{(n_1-1) + (n_2-1)}}
 \end{equation}
<div style="margin-bottom:50px"></div>


One thing to point out is that standard deviations from samples cannot be added together -- but variances can be, which is why in (1) we use $s^2$ in the numerator only to take then the square root of the entire term.

We see that the variances are being multiplied by a term containing the sample size ($n$). In the case that each of our samples were differently sized, we would want to more heavily weight the variance of the sample that was larger. In our case now, both of our samples have the same $n$, but we introduce the equation for more general cases.

If you're wondering why $n-1$ is necessary for the weighting, and not just $n$, hold that thought and we'll return to it later. But for now, we'll just point out that it is *not* to adjust for the inherent bias when calculating sample variance (i.e. not the same reason that we used N-1 in the variance module [link]).

```{r, include=FALSE}
tutorial::go_interactive(height = 160)
```

[DATA CAMP WINDOW WHERE THEY WILL CALCULATE THE S_P OF THE DATA]



# Standard Error (SE) of the mean difference

After calculating the pooled standard deviation, the next step is to determine the standard error of the mean difference. We will follow the same logic as we did when calculated the standard error based on a single sample -- except now we will use the pooled standard deviation. Again, we will be adding the terms from the two samples together, so to be able to sum these we need to use the pooled *variance* (the square of the pooled standard deviation.)

[ annotated?]

<div style="margin-bottom: 50px;"></div>
\begin{equation}
 (\#eq:equation2)
 \Large SE_{({\bar{x_1}-\bar{x_2})}} = \sqrt{\frac{s_p^2}{n_1} + {\frac{s_p^2}{n_2}}}
 \end{equation}
<div style="margin-bottom:50px"></div>


[DATA CAMP WINDOW WHERE THEY WILL CALCULATE THE SE OF THE MEAN DIFF]

Now that you know the standard error, you are ready to construct the 95% CI around the mean difference. As before, this will be the mean difference plus/minus 1.96 * $SE_{({\bar{x_1}-\bar{x_2})}}$. 

Now we can see whether or not our null hypothesis can be rejected. If the 95% CI for values of ${({\bar{x_1}-\bar{x_2})}$ includes 0, then we do not have reason to believe that the population means are different. 

[Construct interval for yourself below, DATA CAMP, INSTRUCTIONS]

```{r, include=FALSE}
tutorial::go_interactive(height = 160)
```

```{r ex="ttest2", type="pre-exercise-code"}

set.seed(12)

heights_island1 <- rnorm(50,10,2)
heights_island2 <- rnorm(50, 18, 1.2)

```

```{r ex="ttest2", type="sample-code"}

# Calculate the mean diff

mean_diff <-  

# Calculate the Sp for heights_island1 and heights_island2
Sp <- 

# Calculate the SE_meandiff
SE_meandiff <- 

# Add ± 1.96 SEM to the sample mean to construct the upper and lower bounds of the 95% CI

upperCI <- 
lowerCI <- 
  
meandiff_95CI <- c(lowerCI, upperCI)
meandiff_95CI
```

```{r ex="ttest2", type="solution"}

# Calculate the mean difference
mean_diff <-  mean(heights_island2) - mean(heights_island1)

# Calculate the Sp for heights_island1 and heights_island2
N1=50
N2=50

Sp <- sqrt(((sd(heights_island1)^2)*(N1-1) + (sd(heights_island2)^2)*(N2-1))/((N1-1)+(N2-1)))

Sp <- sqrt((var(heights_island1)*(49) + var(heights_island2)*(49))/(N1+N2-2))

# Calculate the SE_meandiff

SE_meandiff <- sqrt(Sp^2/N1 + Sp^2/N2)

# Add ± 1.96 SEM to the sample mean to construct the upper and lower bounds of the 95% CI

upperCI <- mean_diff + 1.96*SE_meandiff
lowerCI <-  mean_diff - 1.96*SE_meandiff
  
meandiff_95CI <- c(lowerCI, upperCI)
meandiff_95CI

```

Our 95% CI does not include 0 by a lot! So we can conclude that the population means from Island 1 and Island 2 are mostly likely distinct. In other words, we can reject $H_0$, the null hypothesis. In doing so, we say that the mean difference is *statistically significant*. 

[T test function will calculate the 95CI using the t distribution @ 97.5%m, which doesn't always give 1.96]

# P-values

Even though CIs are great tools for inference, you are probably most familiar with seeing p-values in scientific literature. The **p-value** that is output from your statistical test gives you a metric that tells you whether or not your results are statistically significant. Typically, p-values < 0.05 meet this criterion. 

The "p" in p-value stands for "probability". Probability of what? The **p-value** is the probability that you would have gotten your results or something more extreme if in fact the null hypothesis were true. "More extreme", in this case, would be a mean difference even greater than what we see in the present data. The lower the p-value, the less likely you'd be getting your results due to chance alone.

Now let's derive the p-value for the mean difference of the two island heights, and make sure that we can draw the same conclusion that we did when we used the 95% CI for inference. In order to do this, we will use a statistical test for comparing two groups: the **t-test**.

# The t-test

The t-test is all about the **t-statistic**, which is produced when the mean difference is divided by the standard error. When we divide by the standard error, we turn our mean difference estimate into a unitless metric that is not dependent on the scale that means were recorded with (i.e. centimeters). Furthermore, dividing by the standard error also will also account for the uncertainty in the estimate. 




- The t-statistic is essentially combining whatever our sample estimate is (e.g. mean difference, sample mean, etc.) and its uncertainty (i.e. standard error) into one value.  
- we convert the t-statistic into a p-value via the a specific kind of distribution called the t-distribution. [plot, here's an example of one]
- the further out in the tails of the t-distribution that the t-statistic falls. The lower the pvalue will be. 


(3) Degrees of freedom; Hasse has some ideas.
(4) A formal definition of hypothesis including null hypothesis. 

Implement, and then formally answer the question.

# Degrees of Freedom

- You know the final answer of a single estimate. You can pick whatever you want, until you're down to the last choice....then you don't have the "freedom". --> explains why we use Total -1 = DF.
- Explain Whyyyyyyyyy we need it. 
- The t-distribution will look different depending on the degrees of freedom

Intermediate step here between CI and p-values.
T-test statistic greater than 1.96 will result in a significant p-value.

Write your own t-test function.

Start with same N, but then “to make it a more useful model, you need to be able to handle when the sample sizes are not the same.”

**Unequal variance: more fodder for things to think about**




